{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gym: 0.17.1\n",
      "Tensorflow: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from collections import deque\n",
    "print(\"Gym:\", gym.__version__)\n",
    "print(\"Tensorflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork():\n",
    "    def __init__(self, state_dim, action_size, tau=0.01):\n",
    "        tf.reset_default_graph()\n",
    "        self.state_in = tf.placeholder(tf.float32, shape=[None, *state_dim])\n",
    "        self.action_in = tf.placeholder(tf.int32, shape=[None])\n",
    "        self.q_target_in = tf.placeholder(tf.float32, shape=[None])\n",
    "        self.importance_in = tf.placeholder(tf.float32, shape=[None])\n",
    "        action_one_hot = tf.one_hot(self.action_in, depth=action_size)\n",
    "        \n",
    "        self.q_state_local = self.build_model(action_size, \"local\")\n",
    "        self.q_state_target = self.build_model(action_size, \"target\")\n",
    "        \n",
    "        self.q_state_action = tf.reduce_sum(tf.multiply(self.q_state_local, action_one_hot), axis=1)\n",
    "        self.error = self.q_state_action - self.q_target_in\n",
    "        self.loss = tf.reduce_mean(tf.multiply(tf.square(self.error), self.importance_in))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.loss)\n",
    "        \n",
    "        self.local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"local\")\n",
    "        self.target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target\")\n",
    "        self.updater = tf.group([tf.assign(t, t + tau*(l-t)) for t,l in zip(self.target_vars, self.local_vars)])\n",
    "        \n",
    "    def build_model(self, action_size, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            hidden1 = tf.layers.dense(self.state_in, 100, activation=tf.nn.relu)\n",
    "            q_state = tf.layers.dense(hidden1, action_size, activation=None)\n",
    "            return q_state\n",
    "        \n",
    "    def update_model(self, session, state, action, q_target, importance):\n",
    "        feed = {self.state_in: state, self.action_in: action, self.q_target_in: q_target, self.importance_in: importance}\n",
    "        error, _, _ = session.run([self.error, self.optimizer, self.updater], feed_dict=feed)\n",
    "        return error\n",
    "        \n",
    "    def get_q_state(self, session, state, use_target=False):\n",
    "        q_state_op = self.q_state_target if use_target else self.q_state_local\n",
    "        q_state = session.run(q_state_op, feed_dict={self.state_in: state})\n",
    "        return q_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer():\n",
    "    def __init__(self, maxlen):\n",
    "        self.buffer = deque(maxlen=maxlen)\n",
    "        self.priorities = deque(maxlen=maxlen)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "        \n",
    "    def get_probabilities(self, priority_scale):\n",
    "        scaled_priorities = np.array(self.priorities) ** priority_scale\n",
    "        sample_probabilities = scaled_priorities / sum(scaled_priorities)\n",
    "        return sample_probabilities\n",
    "    \n",
    "    def get_importance(self, probabilities):\n",
    "        importance = 1/len(self.buffer) * 1/probabilities\n",
    "        importance_normalized = importance / max(importance)\n",
    "        return importance_normalized\n",
    "        \n",
    "    def sample(self, batch_size, priority_scale=1.0):\n",
    "        sample_size = min(len(self.buffer), batch_size)\n",
    "        sample_probs = self.get_probabilities(priority_scale)\n",
    "        sample_indices = random.choices(range(len(self.buffer)), k=sample_size, weights=sample_probs)\n",
    "        samples = np.array(self.buffer)[sample_indices]\n",
    "        importance = self.get_importance(sample_probs[sample_indices])\n",
    "        return map(list, zip(*samples)), importance, sample_indices\n",
    "    \n",
    "    def set_priorities(self, indices, errors, offset=0.1):\n",
    "        for i,e in zip(indices, errors):\n",
    "            self.priorities[i] = abs(e) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent():\n",
    "    def __init__(self, env):\n",
    "        self.state_dim = env.observation_space.shape\n",
    "        self.action_size = env.action_space.n\n",
    "        self.q_network = QNetwork(self.state_dim, self.action_size)\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(maxlen=100000)\n",
    "        self.gamma = 0.97\n",
    "        self.eps = 1.0\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        q_state = self.q_network.get_q_state(self.sess, [state])\n",
    "        action_greedy = np.argmax(q_state)\n",
    "        action_random = np.random.randint(self.action_size)\n",
    "        action = action_random if random.random() < self.eps else action_greedy\n",
    "        return action\n",
    "    \n",
    "    def get_env_action(self, action):\n",
    "        return action\n",
    "    \n",
    "    def train(self, state, action, next_state, reward, done, use_DDQN=True, a=0.0):\n",
    "        self.replay_buffer.add((state, action, next_state, reward, done))\n",
    "        (states, actions, next_states, rewards, dones), importance, indices = self.replay_buffer.sample(50, priority_scale=a)\n",
    "        \n",
    "        next_actions = np.argmax(self.q_network.get_q_state(self.sess, next_states, use_target=False), axis=1)\n",
    "        q_next_states = self.q_network.get_q_state(self.sess, next_states, use_target=use_DDQN)\n",
    "        q_next_states[dones] = np.zeros([self.action_size])\n",
    "        q_next_states_next_actions = q_next_states[np.arange(next_actions.shape[0]), next_actions]\n",
    "        q_targets = rewards + self.gamma * q_next_states_next_actions\n",
    "        errors = self.q_network.update_model(self.sess, states, actions, q_targets, importance**(1-self.eps))\n",
    "        \n",
    "        self.replay_buffer.set_priorities(indices, errors)\n",
    "        \n",
    "        if done: self.eps = max(0.1, 0.98*self.eps)\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretizedDQNAgent(DoubleDQNAgent):\n",
    "    def __init__(self, env, n_actions=10):\n",
    "        self.is_discrete = type(env.action_space) == gym.spaces.discrete.Discrete\n",
    "        if not self.is_discrete:\n",
    "            env.action_space.n = n_actions\n",
    "            self.actions = np.linspace(env.action_space.low, env.action_space.high, n_actions)\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def get_env_action(self, action):\n",
    "        if not self.is_discrete:\n",
    "            action = [self.actions[action]]\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(4,)\n",
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env_names = [\"CartPole-v0\",\n",
    "             \"MountainCar-v0\",\n",
    "             \"MountainCarContinuous-v0\",\n",
    "             \"Pendulum-v0\",\n",
    "             \"Acrobot-v1\"]\n",
    "env = gym.make(env_names[0])\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "Episode: 0, total_reward: 19.00\n",
      "Episode: 1, total_reward: 11.00\n",
      "Episode: 2, total_reward: 29.00\n",
      "Episode: 3, total_reward: 25.00\n",
      "Episode: 4, total_reward: 19.00\n",
      "Episode: 5, total_reward: 14.00\n",
      "Episode: 6, total_reward: 57.00\n",
      "Episode: 7, total_reward: 14.00\n",
      "Episode: 8, total_reward: 18.00\n",
      "Episode: 9, total_reward: 12.00\n",
      "Episode: 10, total_reward: 12.00\n",
      "Episode: 11, total_reward: 10.00\n",
      "Episode: 12, total_reward: 9.00\n",
      "Episode: 13, total_reward: 19.00\n",
      "Episode: 14, total_reward: 29.00\n",
      "Episode: 15, total_reward: 21.00\n",
      "Episode: 16, total_reward: 19.00\n",
      "Episode: 17, total_reward: 22.00\n",
      "Episode: 18, total_reward: 9.00\n",
      "Episode: 19, total_reward: 32.00\n",
      "Episode: 20, total_reward: 14.00\n",
      "Episode: 21, total_reward: 15.00\n",
      "Episode: 22, total_reward: 10.00\n",
      "Episode: 23, total_reward: 12.00\n",
      "Episode: 24, total_reward: 27.00\n",
      "Episode: 25, total_reward: 16.00\n",
      "Episode: 26, total_reward: 31.00\n",
      "Episode: 27, total_reward: 10.00\n",
      "Episode: 28, total_reward: 10.00\n",
      "Episode: 29, total_reward: 12.00\n",
      "Episode: 30, total_reward: 66.00\n",
      "Episode: 31, total_reward: 57.00\n",
      "Episode: 32, total_reward: 68.00\n",
      "Episode: 33, total_reward: 43.00\n",
      "Episode: 34, total_reward: 97.00\n",
      "Episode: 35, total_reward: 72.00\n",
      "Episode: 36, total_reward: 133.00\n",
      "Episode: 37, total_reward: 64.00\n",
      "Episode: 38, total_reward: 49.00\n",
      "Episode: 39, total_reward: 60.00\n",
      "Episode: 40, total_reward: 66.00\n",
      "Episode: 41, total_reward: 49.00\n",
      "Episode: 42, total_reward: 49.00\n",
      "Episode: 43, total_reward: 107.00\n",
      "Episode: 44, total_reward: 105.00\n",
      "Episode: 45, total_reward: 169.00\n",
      "Episode: 46, total_reward: 200.00\n"
     ]
    }
   ],
   "source": [
    "num_runs = 1\n",
    "run_rewards = []\n",
    "\n",
    "for n in range(num_runs):\n",
    "    print(\"Run {}\".format(n))\n",
    "    ep_rewards = []\n",
    "    agent = None\n",
    "    agent = DiscretizedDQNAgent(env)\n",
    "    num_episodes = 100\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(agent.get_env_action(action))\n",
    "            agent.train(state, action, next_state, reward, done, a=0.7)\n",
    "            env.render()\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        ep_rewards.append(total_reward)\n",
    "        print(\"Episode: {}, total_reward: {:.2f}\".format(ep, total_reward))\n",
    "        \n",
    "    run_rewards.append(ep_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
